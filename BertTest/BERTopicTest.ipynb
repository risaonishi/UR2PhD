{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/4pnxxp31713_7zs16j53z8w40000gp/T/ipykernel_71502/1878901361.py:2: DtypeWarning: Columns (0,2,8,9,10,12,14,16,17,19,20,22,25,28,31,38,43,44,45,48,50,57,61,63,64,67,69,70,71,73,82,83,88,89,90,93,96,97,99,101,102,103,104,105,106,107,108,109,111,113,114,116,117,118,120,123,124,125,127,129,130) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../Data/ucr_submissions.csv\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced3d9601f7a488f9d4c14565d1dc06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                selftext  cluster\n",
      "52028  I found Abigail Guadalupe Castillo Hernandez's...       37\n",
      "28955  Does anyone have any recommendations for apart...       16\n",
      "27070  Hey is anyone by the name of Alexander missing...       37\n",
      "2400   I went to the rec center and there are barely ...       30\n",
      "39011  Lost my keys on campus yesterday. If anybody h...       37\n",
      "cluster\n",
      "-1     362\n",
      " 16    101\n",
      " 32     31\n",
      " 13     29\n",
      " 36     28\n",
      " 12     23\n",
      " 37     18\n",
      " 44     18\n",
      " 26     17\n",
      " 54     15\n",
      " 6      14\n",
      " 56     14\n",
      " 14     14\n",
      " 52     13\n",
      " 51     12\n",
      " 39     12\n",
      " 45     11\n",
      " 50     11\n",
      " 25     11\n",
      " 27     10\n",
      " 8      10\n",
      " 15     10\n",
      " 41     10\n",
      " 35      9\n",
      " 34      9\n",
      " 29      9\n",
      " 21      8\n",
      " 58      8\n",
      " 20      7\n",
      " 43      7\n",
      " 18      7\n",
      " 42      7\n",
      " 24      7\n",
      " 33      7\n",
      " 11      6\n",
      " 5       6\n",
      " 48      6\n",
      " 23      6\n",
      " 1       6\n",
      " 7       6\n",
      " 2       6\n",
      " 0       6\n",
      " 49      5\n",
      " 19      5\n",
      " 46      5\n",
      " 30      5\n",
      " 57      5\n",
      " 4       4\n",
      " 22      4\n",
      " 38      4\n",
      " 3       4\n",
      " 40      4\n",
      " 55      4\n",
      " 9       4\n",
      " 10      4\n",
      " 53      4\n",
      " 28      3\n",
      " 31      3\n",
      " 47      3\n",
      " 17      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and filter data\n",
    "df = pd.read_csv(\"../Data/ucr_submissions.csv\")\n",
    "df_filtered = df[df['selftext'].notna()]\n",
    "df_filtered = df_filtered[~df_filtered['selftext'].isin(['[deleted]', '[removed]'])]\n",
    "df_sample = df_filtered.sample(n=1000, random_state=42)\n",
    "\n",
    "# 1: Extract 'selftext' column as a list for clustering\n",
    "docs = df_sample['selftext'].tolist()\n",
    "\n",
    "# 2: Generate BERT embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# 3: Dimensionality reduction with UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=30, n_components=5, metric='cosine', random_state=42)\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# 4: Clustering with HDBSCAN\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=3, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "clusters = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "\n",
    "# 5: Add clusters as a new column to the DataFrame\n",
    "df_sample['cluster'] = clusters\n",
    "df_clusters = df_sample[['selftext', 'cluster']]\n",
    "# Inspect the results\n",
    "print(df_clusters.head())        # Check the first few rows\n",
    "print(df_clusters['cluster'].value_counts())  # Count documents in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster                                  concatenated_text\n",
      "0       -1  It says the PSYC012 lecture with John Franchak...\n",
      "1        0  Does that mean itâ€™s over zoom? Anyone else reg...\n",
      "2        1  Title\\n\\nAny help is appreciated The title has...\n",
      "3        2  [deleted]\\n\\n[View Poll](https://www.reddit.co...\n",
      "4        3  &#x200B;\\n\\nhttps://preview.redd.it/sryzjn29aw...\n"
     ]
    }
   ],
   "source": [
    "# 6: Concatenate documents in each cluster\n",
    "clustered_documents = df_clusters.groupby('cluster')['selftext'].apply(' '.join).reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "clustered_documents.rename(columns={'selftext': 'concatenated_text'}, inplace=True)\n",
    "\n",
    "# Inspect the concatenated documents\n",
    "print(clustered_documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Calcualte c-TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')  \n",
    "\n",
    "# Fit and transform the concatenated documents for each cluster\n",
    "tfidf_matrix = vectorizer.fit_transform(clustered_documents['concatenated_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for easier interpretation\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=clustered_documents['cluster'], columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        top_word_1     top_word_2    top_word_3              top_word_4  \\\n",
      "cluster                                                                   \n",
      "0             zoom         canvas         email                 checked   \n",
      "1            title           yeah           pls                 advance   \n",
      "2             poll           view           www                     com   \n",
      "3              png          https       preview                    redd   \n",
      "4            right           girl       hearing                 careful   \n",
      "5           honors        program         apply                   worth   \n",
      "6            tired         school        online                    life   \n",
      "7             wifi        connect    connecting                 printer   \n",
      "8        decisions         portal        gotten                    know   \n",
      "9           shower        lecture    understand                    warn   \n",
      "10           media            abt       context                     ppl   \n",
      "11          stolen        scooter     literally                  police   \n",
      "12         parking         permit          park                     lot   \n",
      "13         housing         campus          rent              apartments   \n",
      "14          campus         coffee            la                 weekend   \n",
      "15       textbooks            pdf      textbook                 edition   \n",
      "16           lease           room     apartment                    rent   \n",
      "17          campus            car          near                  picked   \n",
      "18          photos           grad         https                  friend   \n",
      "19            card         dining          hall                    does   \n",
      "20          campus          covid        return                     ucr   \n",
      "21           notes             gt        course  universityofcalifornia   \n",
      "22            code             35     reference                   click   \n",
      "23       graduated    transcripts       diploma               completed   \n",
      "24             ucr          event           run                    sure   \n",
      "25        register   registration  immunization                   holds   \n",
      "26           error         ilearn           amp                    rweb   \n",
      "27             aid         summer     financial                 classes   \n",
      "28         english         credit       bus100w                 instead   \n",
      "29         spanish       language     placement                    test   \n",
      "30             gym       machines           use                  female   \n",
      "31         summary         ucship        health                   miles   \n",
      "32             aid      financial         award                   grant   \n",
      "33         friends    programming       looking                     new   \n",
      "34            meet       location       tonight                      ll   \n",
      "35          campus           club       friends                    join   \n",
      "36         tickets          extra        ticket                 willing   \n",
      "37            lost           keys         black                    left   \n",
      "38         discord         abroad        server                      gg   \n",
      "39         discord            lab          chem                  canvas   \n",
      "40         biochem            lab         major                incoming   \n",
      "41       interview         prompt         legit                    know   \n",
      "42            math           calc          test                   major   \n",
      "43         classes          units          easy                 quarter   \n",
      "44             gpa        quarter         units                   class   \n",
      "45        withdraw            106         class                 advisor   \n",
      "46         quarter           divs       classes                   class   \n",
      "47              uc            cal       tuition                 housing   \n",
      "48        syllabus      professor         class                  course   \n",
      "49          course        breadth      division                register   \n",
      "50            chem          units           lab                    biol   \n",
      "51            chem          class          labs                     lab   \n",
      "52         midterm          class          exam                   study   \n",
      "53       professor       lectures          does        ratemyprofessors   \n",
      "54           class      professor         taken                  taking   \n",
      "55        business          major        course                engineer   \n",
      "56              uc  opportunities           ucr                transfer   \n",
      "57            poli            sci        public                   major   \n",
      "58          switch             cs         major                  change   \n",
      "\n",
      "          top_word_5  \n",
      "cluster               \n",
      "0         discussion  \n",
      "1           question  \n",
      "2             reddit  \n",
      "3              width  \n",
      "4               warn  \n",
      "5           priority  \n",
      "6               feel  \n",
      "7             doesnt  \n",
      "8           waitlist  \n",
      "9            yelling  \n",
      "10            people  \n",
      "11              cars  \n",
      "12               car  \n",
      "13         apartment  \n",
      "14              ones  \n",
      "15              book  \n",
      "16               bed  \n",
      "17       necessarily  \n",
      "18               com  \n",
      "19           problem  \n",
      "20              said  \n",
      "21              html  \n",
      "22               use  \n",
      "23          official  \n",
      "24              cold  \n",
      "25            health  \n",
      "26               fix  \n",
      "27           quarter  \n",
      "28                1c  \n",
      "29           foreign  \n",
      "30           staying  \n",
      "31         insurance  \n",
      "32            office  \n",
      "33             learn  \n",
      "34              http  \n",
      "35               amp  \n",
      "36                pm  \n",
      "37              case  \n",
      "38             https  \n",
      "39             kevin  \n",
      "40           writing  \n",
      "41          benefits  \n",
      "42               pre  \n",
      "43              unit  \n",
      "44          honestly  \n",
      "45            course  \n",
      "46             upper  \n",
      "47            income  \n",
      "48               ask  \n",
      "49            enroll  \n",
      "50            course  \n",
      "51             kevin  \n",
      "52            advice  \n",
      "53             style  \n",
      "54            course  \n",
      "55            advise  \n",
      "56                sb  \n",
      "57           science  \n",
      "58           science  \n"
     ]
    }
   ],
   "source": [
    "# Filter out cluster -1 from tfidf_df\n",
    "tfidf_df_no_noise = tfidf_df[tfidf_df.index != -1]\n",
    "\n",
    "# Get the top 5 words for each cluster based on TF-IDF scores\n",
    "top_n = 5\n",
    "top_words = {}\n",
    "\n",
    "# Loop through each cluster (excluding -1) to get the top words\n",
    "for cluster in tfidf_df_no_noise.index:\n",
    "    sorted_words = tfidf_df_no_noise.loc[cluster].sort_values(ascending=False).head(top_n)\n",
    "    top_words[cluster] = list(sorted_words.index)\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easy viewing\n",
    "top_words_df = pd.DataFrame.from_dict(top_words, orient='index', columns=[f'top_word_{i+1}' for i in range(top_n)])\n",
    "top_words_df.index.name = 'cluster'\n",
    "\n",
    "# Display the top 5 TF-IDF words for each cluster (excluding -1)\n",
    "print(top_words_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8:Apply NFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix created with n-grams 2.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english', ngram_range=(2,2))\n",
    "tfidf_matrix = vectorizer.fit_transform(clustered_documents['concatenated_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for easier interpretation (optional)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=clustered_documents['cluster'], columns=vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF matrix created with n-grams 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF applied with Frobenius norm objective function.\n"
     ]
    }
   ],
   "source": [
    "# Set the number of topics you want to extract\n",
    "n_topics = 4\n",
    "# Initialize and fit NMF\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42, init='random', solver='mu', beta_loss='frobenius', max_iter=200)\n",
    "W = nmf_model.fit_transform(tfidf_matrix)  # Document-topic matrix\n",
    "H = nmf_model.components_  # Topic-term matrix\n",
    "\n",
    "print(\"NMF applied with Frobenius norm objective function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: financial aid | work study | campus housing | cal grant | summer classes\n",
      "Topic #2: don want | don know | does know | help appreciated | computer science\n",
      "Topic #3: reddit com | www reddit | https www | com poll | view poll\n",
      "Topic #4: https preview | preview redd | auto webp | png width | format png\n"
     ]
    }
   ],
   "source": [
    "# 9: Get the feature names (terms) from the TF-IDF vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 5\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_terms = [terms[i] for i in topic.argsort()[-n_top_words:]][::-1]\n",
    "    print(f\"Topic #{topic_idx+1}: {' | '.join(top_terms)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ur2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
